{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f18f19-ea83-4b53-9fc1-b111e9cb491b",
   "metadata": {},
   "source": [
    "# AGNFinder CVAE Framework Test (1)\n",
    "\n",
    "**Author:** Maxime Robeyns (2021) <maximerobeyns@gmail.com>\n",
    "\n",
    "**Digit Generation**\n",
    "\n",
    "In this notebook, we perform conditional handwritten digit generation to test the CVAE implementation in the AGNFinder codebase. We use the MNIST dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd80a5cd-4274-41f5-be55-68f224cf2f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import Union\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import agnfinder.inference as inf\n",
    "import agnfinder.inference.distributions as dist\n",
    "\n",
    "from agnfinder import config as cfg\n",
    "from agnfinder.types import DistParams, arch_t\n",
    "from agnfinder.inference.utils import _load_mnist, _onehot, Squareplus\n",
    "from agnfinder.inference.base import CVAE, CVAEPrior, CVAEEnc, CVAEDec, \\\n",
    "                                     CVAEParams, _CVAE_Dist, _CVAE_RDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31083021-5795-4e37-9801-5701606ffe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # One-time setup\n",
    "    assert(_SETUP)\n",
    "except NameError:\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore', category=UserWarning)  # see torchvision pr #4184\n",
    "    cfg.configure_logging()\n",
    "    while not '.git' in os.listdir():\n",
    "        os.chdir(\"../\")\n",
    "    dtype = t.float64\n",
    "    device = t.device(\"cuda\") if t.cuda.is_available() else t.device(\"cpu\")\n",
    "    if device == t.device(\"cuda\"):\n",
    "        print('Using GPU for training.')\n",
    "        # !nvidia-smi\n",
    "    else:\n",
    "        print('CUDA is unavailable; training on CPU.')\n",
    "    _SETUP = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969ad345-0972-4211-a5dd-21e1e8a3493a",
   "metadata": {},
   "source": [
    "This class is called at every logging iteration, and just visualises the current state of the decoder network by conditioning on digits $x = \\{0, \\ldots, 9\\}$ and plotting the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8de9c0-b21f-44bb-81b1-25e51f4128cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogVisual():\n",
    "    def __init__(self, figure, display):\n",
    "        self.fig = figure\n",
    "        self.disp = display\n",
    "        self.ax = figure.subplots(1, 10)\n",
    "        for i in range(10):\n",
    "            self.ax[i].axis('off')\n",
    "            self.ax[i].set_title(f'x={i}', fontsize=15, color=(.5, .5, .5))\n",
    "\n",
    "    def __call__(self, cvae: CVAE):\n",
    "        with t.no_grad():\n",
    "            x = t.eye(10, device=cvae.device, dtype=cvae.dtype)\n",
    "            z = cvae.prior.get_dist(None).sample((10,))\n",
    "            ys, _ = cvae.decoder.forward(t.cat((z, x), -1))\n",
    "            for i in range(10):\n",
    "                self.ax[i].imshow(-ys[i].view(28, 28).cpu().data.numpy(), \n",
    "                                  cmap=plt.get_cmap('PuBu'))\n",
    "            self.fig.tight_layout()\n",
    "            self.disp.update(self.fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcc800b-96fc-473d-9995-2f0ce197a23a",
   "metadata": {},
   "source": [
    "## CVAE Setup\n",
    "\n",
    "Recall that our training objective, the ELBO, can be expressed as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}_{\\text{CVAE}}(\\theta, \\phi; x, y) &= \n",
    "    \\mathbb{E}_{q_{\\phi}(z \\vert y, x)}\\left[\\log p_{\\theta}(y \\vert z, x)\\right]\n",
    "     - D_{\\text{KL}}\\left[q_{\\phi}(z \\vert y, x) \\Vert p_{\\theta}(z \\vert x)\\right] \\\\\n",
    "       &= \\mathbb{E}_{q_{\\phi}(z \\vert y, x)}\\big[\\log p_{\\theta}(y \\vert z, x) + \\log p_{\\theta}(z \\vert x) - \\log q_{\\phi}(z \\vert y, x)\\big] \\\\\n",
    "       &\\doteq \\mathbb{E}\\big[\\mathcal{L}_{\\text{logpy}} +\n",
    "       \\mathcal{L}_{\\text{logpz}} - \\mathcal{L}_{\\text{logqz}} \\big].\n",
    "\\end{align*}\n",
    "\n",
    "Here we will select a standard Gaussian prior $p_{\\theta}(z \\vert x) = \\mathcal{N}(z; 0, \\mathbf{I})$ (that is, we do not condition this distribution on $x$), which we can get from the `StandardGaussianPrior` distribution implemented in `agnfinder.inference`.\n",
    "\n",
    "For the encoder network $q_{\\phi}(z \\vert y, x)$, we select a full-covariance Gaussian, which we get from `agnfinder.inference.GaussianEncoder`. This implements reparametrised sampling behind the scenes (using the `agnfinder.inference.distributions.R_MVN` distribution).\n",
    "\n",
    "The decoder $p_{\\theta}(y \\vert z, x)$ is a factorised Gaussian; it needn't use reparametrised sampling—indeed, we never sample from it in the CVAE training procedure, however even if we did it wouldn't be beneficial nor detrimental. We get it from `agnfinder.inference.FactorisedGaussianDecoder`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d98f749-9c88-4e2b-9c62-a8ab69a3210a",
   "metadata": {},
   "source": [
    "In parametrising the CVAE, we set the `cond_dim` to 10, since we are conditioning on the one-hot encoded digits $x \\in \\{0, \\ldots, 9\\}$. The `data_dim` is $784 = 28\\times 28$ to reflect the size of a flattened MNIST image. \n",
    "\n",
    "The `latent_dim` is arbitrarily chosen; make this too large and the representations we learn will suffer, since the VAE has little incentive to compress an input image efficiently in order to acheive a good reconstruction. Make this too small, and the network may struggle to retain enough information in the latent code to reconstruct the input again effectively (although this is unlikely for this toy MNIST example, even at $\\mathtt{latent\\_dim} = 1$).\n",
    "\n",
    "Since the `StandardGaussianPrior` doesn't use the conditioning information $x$ provided to it, we use a `prior_arch = None` to avoid instantiating an unnecessary network.\n",
    "\n",
    "For the encoder and decoder, the number of output 'heads' matches the number of parameters that the corresponding distribution is expecting. For the decoder, we use a sigmoid activation on the mean since the MNIST pixel data lies in $y \\in [0, 1]^{28\\times 28}$. (See the docstrings on the distributions implemented in the `agnfinder.inference.inference.py` file to see example—that is, compatible—architectures.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386621f9-0bd2-4ed8-8b41-646d25032e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_img_params(CVAEParams):\n",
    "    cond_dim = 10  # x; dimension of one-hot labels\n",
    "    data_dim = 28*28  # y; size of MNIST image\n",
    "    latent_dim = 2  # z\n",
    "    adam_lr = 1e-3\n",
    "\n",
    "    prior = inf.StandardGaussianPrior\n",
    "    prior_arch = None\n",
    "\n",
    "    # full-covariance\n",
    "    encoder = inf.GaussianEncoder\n",
    "    enc_arch = arch_t([data_dim + cond_dim, 256], [latent_dim, latent_dim, latent_dim*latent_dim], \n",
    "                      nn.SiLU(), [None, Squareplus(0.8), Squareplus(0.8)], batch_norm=True)\n",
    "\n",
    "    decoder = inf.FactorisedGaussianDecoder\n",
    "    dec_arch = arch_t([latent_dim + cond_dim, 256], [data_dim, data_dim],\n",
    "                      nn.SiLU(), [nn.Sigmoid(), Squareplus(0.8)], batch_norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0db2648-d80b-44b7-8d9d-fef2527db976",
   "metadata": {},
   "source": [
    "Here we minimally modify the base CVAE class by overriding the base `preprocess` method to work with the MNIST data.\n",
    "\n",
    "Since MNIST is usually used for classification tasks, the data coming out of  the data loader will have $x$ as the pixel data, and $y$ as the corresponding integer MNIST labels.\n",
    "\n",
    "However, here we want $x$ to be one-hot encoded labels, and $y$ to be the flattened pixel data; so we apply the necessary transformations in this function, and switch the order of the two before returning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1022e8f8-00dc-4b0e-b951-8b9fc4309e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_img_cvae(CVAE):\n",
    "\n",
    "    def preprocess(self, x: t.Tensor, y: t.Tensor) -> tuple[t.Tensor, t.Tensor]:\n",
    "        if x.dim() > 2:\n",
    "            x = x.view(-1, 28*28)\n",
    "        switched_x = _onehot(y, 10).to(self.device, self.dtype)\n",
    "        switched_y = x.to(self.device, self.dtype)\n",
    "        return switched_x, switched_y\n",
    "    \n",
    "    # trainmodel and ELBO methods kept as in base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b7ae07-28d0-46c9-9de6-113e56e380be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 2.5))\n",
    "disp = display.display(\"\", display_id=True)\n",
    "\n",
    "cvae = MNIST_img_cvae(MNIST_img_params(), device=device, dtype=dtype, \n",
    "                      logging_callbacks=[LogVisual(fig, disp)])\n",
    "train_loader, _ = _load_mnist()\n",
    "\n",
    "cvae.trainmodel(train_loader, epochs=5)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d397c880-aa5a-416c-bf3e-f5e3caa186f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agnvenv (Python 3.9)",
   "language": "python",
   "name": "agnvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
