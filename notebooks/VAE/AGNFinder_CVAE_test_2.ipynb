{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f18f19-ea83-4b53-9fc1-b111e9cb491b",
   "metadata": {},
   "source": [
    "# AGNFinder CVAE Framework Test (2)\n",
    "\n",
    "**Author:** Maxime Robeyns (2021) <maximerobeyns@gmail.com>\n",
    "\n",
    "**Digit Classification**\n",
    "\n",
    "In this notebook, we use the AGNFinder CVAE as a 'classifier' or discriminative model, by conditioning on pixel data, and generating the most likely digit label. This complements the first test where we conditioned on the one-hot encoded labels rather than pixel data.\n",
    "\n",
    "This is merely intended to test the CVAE implementation and demonstrate that it works correctly; not reach SOTA on MNIST classification which is awefully *passÃ©*..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd80a5cd-4274-41f5-be55-68f224cf2f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import Type\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import agnfinder\n",
    "import agnfinder.config as cfg\n",
    "import agnfinder.inference as inf\n",
    "\n",
    "from agnfinder.types import arch_t\n",
    "from agnfinder.inference import cvae\n",
    "from agnfinder.inference.utils import _load_mnist, _onehot, Squareplus\n",
    "\n",
    "try: # One-time setup\n",
    "    assert(_SETUP)\n",
    "except NameError:\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore', category=UserWarning)  # see torchvision pr #4184\n",
    "    cfg.configure_logging()\n",
    "    os.chdir(os.path.split(agnfinder.__path__[0])[0])\n",
    "    device = t.device(\"cuda\") if t.cuda.is_available() else t.device(\"cpu\")\n",
    "    if device == t.device(\"cuda\"):\n",
    "        print('Using GPU for training.')\n",
    "        # !nvidia-smi\n",
    "    else:\n",
    "        print('CUDA is unavailable; training on CPU.')\n",
    "    _SETUP = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969ad345-0972-4211-a5dd-21e1e8a3493a",
   "metadata": {},
   "source": [
    "This ugly class creates a pretty visualisation of the test accuracy and some classification examples during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8de9c0-b21f-44bb-81b1-25e51f4128cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogVisual():\n",
    "    def __init__(self, fig, display, test_loader):\n",
    "        self.fig = fig\n",
    "        self.gs = self.fig.add_gridspec(2, 10)\n",
    "        self.testset = test_loader.dataset\n",
    "        self.disp = display\n",
    "        self.evals = []  # TODO include uncertainty estimates\n",
    "\n",
    "    def __call__(self, cvae: cvae.CVAE):\n",
    "        self.fig.clear()\n",
    "        with t.no_grad():\n",
    "            bs = 1000\n",
    "            ts = Subset(self.testset, t.randint(len(self.testset), (bs,)))\n",
    "            ax1 = self.fig.add_subplot(self.gs[0, :])\n",
    "            ax1.set_xlabel('Logging Iteration')\n",
    "            ax1.set_ylabel('Classification Accuracy')\n",
    "            ax1.set_ylim(0.5, 1.)\n",
    "            for x, y in DataLoader(ts, batch_size=bs):\n",
    "                x = x.view(-1, 28*28).to(cvae.device, cvae.dtype)\n",
    "                y = y.to(cvae.device, cvae.dtype)\n",
    "                z = cvae.prior.get_dist(None).sample((bs,))\n",
    "                ys = cvae.decoder.forward(t.cat((z, x), -1)).argmax(1)\n",
    "                acc = (t.sum(y == ys) / bs).item()\n",
    "                self.evals.append(acc)\n",
    "                ax1.set_title(f'{bs} sample test accuracy: {acc*100:.2f}%', color='black')\n",
    "                ax1.plot(self.evals)\n",
    "                for i in range(10):\n",
    "                    ax_tmp = self.fig.add_subplot(self.gs[1, i])\n",
    "                    ax_tmp.axis('off')\n",
    "                    ax_tmp.imshow(-x[i].view(28, 28).cpu().data.numpy(), \n",
    "                                  cmap=plt.get_cmap('PuBu'))\n",
    "                    ax_tmp.set_title(f'prediction: {ys[i]}', fontsize=15, color='black')\n",
    "            self.fig.tight_layout()\n",
    "            self.disp.update(self.fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcc800b-96fc-473d-9995-2f0ce197a23a",
   "metadata": {},
   "source": [
    "## CVAE Setup\n",
    "\n",
    "The ELBO is\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}_{\\text{CVAE}}(\\theta, \\phi; x, y) &= \n",
    "    \\mathbb{E}_{q_{\\phi}(z \\vert y, x)}\\left[\\log p_{\\theta}(y \\vert z, x)\\right]\n",
    "     - D_{\\text{KL}}\\left[q_{\\phi}(z \\vert y, x) \\Vert p_{\\theta}(z \\vert x)\\right] \\\\\n",
    "       &= \\mathbb{E}_{q_{\\phi}(z \\vert y, x)}\\big[\\log p_{\\theta}(y \\vert z, x) + \\log p_{\\theta}(z \\vert x) - \\log q_{\\phi}(z \\vert y, x)\\big] \\\\\n",
    "       &\\doteq \\mathbb{E}\\big[\\mathcal{L}_{\\text{logpy}} +\n",
    "       \\mathcal{L}_{\\text{logpz}} - \\mathcal{L}_{\\text{logqz}} \\big].\n",
    "\\end{align*}\n",
    "\n",
    "Here we use a standard Gaussian prior which we do not condition on anything; $p_{\\theta}(z \\vert x) = \\mathcal{N}(z; 0, \\mathbf{I})$. We also use a factorised Gaussian for the recognition model $q_{\\phi}(z \\vert y, x)$.\n",
    "\n",
    "For the generator model $p_{\\theta}(y \\vert z, x)$, we use a multinomial distribution, which is implemented in `agnfinder.inference.cvae_dist` as a thin wrapper around the native PyTorch `Multinomial` distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d98f749-9c88-4e2b-9c62-a8ab69a3210a",
   "metadata": {},
   "source": [
    "The parameters of this CVAE are fairly similar to the first test, however now the `cond_dim` is 784 since we are conditioning on the pixel data to obtain our 'discriminative model'. The `data_dim` is 10 for one-hot encoded image labels.\n",
    "\n",
    "We use a softmax activation function on the output of the decoder network, since this is used to parametrise the multinomial distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1022e8f8-00dc-4b0e-b951-8b9fc4309e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_label_cvae(cvae.CVAE):\n",
    "    \n",
    "    name: str = 'MNIST label CVAE'\n",
    "    \n",
    "    def fpath(self) -> str:\n",
    "        if self.savepath_cached == '':\n",
    "            self.savepath_cached = './results/nbresults/MNIST_label_cvae.pt'\n",
    "        return self.savepath_cached\n",
    "\n",
    "    def preprocess(self, x: t.Tensor, y: t.Tensor) -> tuple[t.Tensor, t.Tensor]:\n",
    "        if x.dim() > 2:\n",
    "            x = x.view(-1, 28*28)\n",
    "        x = x.to(self.device, self.dtype)\n",
    "        y = _onehot(y, 10).to(self.device, self.dtype)\n",
    "        return x, y\n",
    "    \n",
    "    # trainmodel and ELBO methods kept as in base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386621f9-0bd2-4ed8-8b41-646d25032e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_label_params(cvae.CVAEParams):\n",
    "    epochs: int = 3\n",
    "    batch_size: int = 512\n",
    "    dtype: t.dtype = t.float32\n",
    "    adam_lr: float = 1e-3\n",
    "    \n",
    "    cond_dim: int = 28*28  # x; dimension of MNIST image pixel data\n",
    "    data_dim: int = 10  # y; size of one-hot encoded digit labels\n",
    "    latent_dim: int = 2  # z\n",
    "\n",
    "    prior: Type[cvae.CVAEPrior] = inf.StandardGaussianPrior\n",
    "    prior_arch: arch_t = None\n",
    "\n",
    "    encoder: Type[cvae.CVAEEnc] = inf.FactorisedGaussianEncoder\n",
    "    enc_arch: arch_t = arch_t([data_dim + cond_dim, 256], [latent_dim, latent_dim],\n",
    "                      nn.SiLU(), batch_norm=True)\n",
    "\n",
    "    decoder: Type[cvae.CVAEDec] = inf.MultinomialDecoder\n",
    "    dec_arch: arch_t = arch_t([latent_dim + cond_dim, 256], [data_dim],\n",
    "                      nn.SiLU(), [nn.Softmax()], batch_norm=True)\n",
    "    \n",
    "class InferenceParams(inf.InferenceParams):\n",
    "    model: cvae.CVAE = MNIST_label_cvae\n",
    "    retrain_model: bool = True\n",
    "    overwrite_results: bool = True\n",
    "    logging_frequency: int = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b7ae07-28d0-46c9-9de6-113e56e380be",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = plt.figure(figsize=(20, 5), dpi=200, facecolor=(1, 1., 1., .8))\n",
    "disp = display.display(\"\", display_id=True)\n",
    "\n",
    "mp = MNIST_label_params()\n",
    "ip = InferenceParams()\n",
    "\n",
    "train_loader, test_loader = _load_mnist(mp.batch_size)\n",
    "model = MNIST_label_cvae(mp, logging_callbacks=[LogVisual(f1, disp, test_loader)])\n",
    "\n",
    "model.trainmodel(train_loader, ip)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agnvenv (Python 3.9)",
   "language": "python",
   "name": "agnvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
