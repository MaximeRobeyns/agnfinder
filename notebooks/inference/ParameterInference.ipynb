{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Inference\n",
    "\n",
    "\n",
    "This notebook contains various experiments and model comparisons relating to inferring physical galaxy parameters $y$ from photometric observations $x$.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Data Visualisation](#Data-Distributions)\n",
    "- [Single-variable experiments](#Gaussians-for-Individual-Parameters)\n",
    "- [Beyond 1 dimension](#Multivariate-Mixture-Distributions)\n",
    "- [\"Sequential Autoregressive Network\"](#\"Sequential-Autoregressive-Network\"-(SAN))\n",
    "- [Masked Autoencoder for Distribution Estimation](#MADE) (buggy, do not run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import corner\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import agnfinder\n",
    "import agnfinder.inference as inference\n",
    "import agnfinder.inference.made as made\n",
    "\n",
    "from IPython.display import SVG, display\n",
    "from typing import Type\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.distributions import Normal, MultivariateNormal, StudentT, Laplace, MixtureSameFamily, Categorical\n",
    "\n",
    "from agnfinder import config as cfg\n",
    "from agnfinder import nbutils as nbu\n",
    "from agnfinder.types import ConfigClass, column_order\n",
    "from agnfinder.inference import CMADE, SAN\n",
    "from agnfinder.inference.made import MaskedLinear\n",
    "from agnfinder.inference.base import CVAE, CVAEParams, cvae_t, arch_t\n",
    "from agnfinder.inference.utils import Squareplus, squareplus_f, load_simulated_data, GalaxyDataset\n",
    "from agnfinder.simulation.utils import denormalise_theta, normalise_theta\n",
    "\n",
    "try: # One-time setup\n",
    "    assert(_SETUP)\n",
    "except NameError:\n",
    "    cfg.configure_logging()\n",
    "    os.chdir(os.path.split(agnfinder.__path__[0])[0])\n",
    "    dtype = t.float32\n",
    "    device = t.device(\"cuda\") if t.cuda.is_available() else t.device(\"cpu\")\n",
    "    if device == t.device(\"cuda\"):\n",
    "        print(f'Using GPU for training')\n",
    "        # !nvidia-smi\n",
    "    else:\n",
    "        print(\"CUDA is unavailable; training on CPU.\")\n",
    "    _SETUP = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Distributions\n",
    "\n",
    "Finding the distribution $p_{\\theta}(y \\vert x)$ is akin to finding a one-to-many mapping $f: \\mathcal{X} \\times \\Theta \\to \\mathcal{Y}$ from photometric observations to physical galaxy parameters.\n",
    "\n",
    "Before doing any inference, it might be wise to visualise the empirical distribution of photometric measurements $\\mathcal{X}$ and physical galaxy parameters $\\mathcal{Y}$ that we have in our (simulated) training dataset. \n",
    "\n",
    "This can help us to tell whether the required mapping $f$ is very complicated and non-linear or whether a linear combination of the photometric measurements will do (I suspect it's the former). We can also see whether the data is nicely normalised and further whether it is nicely distributed within its normalised range (e.g. $[0,1]$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_phot_np(x: np.ndarray) -> np.ndarray:\n",
    "    x_log = np.log(x)\n",
    "    return (x_log - x_log.mean(0)) / x_log.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loc: str = './data/cubes/latest_sample/'\n",
    "gd = GalaxyDataset(path=dataset_loc, transforms=[transforms.ToTensor()])\n",
    "fp = cfg.FreeParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Photometry distribution, $\\mathcal{X}$\n",
    "\n",
    "Let's first visualise the outputs of the forward model (and the inputs to our mapping).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = gd.get_xs().numpy()\n",
    "small_xs = xs[:1000]\n",
    "labels_x = [f'dimension {i}' for i in range(len(xs[0]))]\n",
    "nbu.plot_corner(small_xs, title='Photometry Distribution', \n",
    "                description='Un-normalised outputs from the simulation process',\n",
    "                labels=labels_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data points have a huge range, and that they follow a log-scale too. \n",
    "\n",
    "Taking the logarithm of the $x$ points, and applying z-score normalisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm = normalise_phot_np(xs[:10000])\n",
    "nbu.plot_corner(x_norm, title='Normalised Photometric Observations', \n",
    "                description='x-samples are passed through a logarithm before applying z-score normalisation', \n",
    "                labels=labels_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physical Galaxy Parameter Distribution, $\\mathcal{Y}$\n",
    "\n",
    "Recall that we performed Latin-hypercube sampling to obtain the physical parameter values that we provided to the forward model. Due of this sampling procedure, we would expect their (prior) distribution to be roughly uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = gd.get_ys().numpy()[:10000]\n",
    "nbu.plot_corner(ys, title=\"Physical Galaxy Parameters\",\n",
    "                description=\"Normalised values, as obtained from LHS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some setup for inference later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceParams(ConfigClass):\n",
    "    epochs: int = 5\n",
    "    batch_size: int = 512\n",
    "    split_ratio: float = 0.9\n",
    "    dtype: t.dtype = dtype\n",
    "    device: t.device = device\n",
    "    logging_frequency: int = 10000\n",
    "    dataset_loc: str = './data/cubes/latest_sample/'\n",
    "    retrain_model: bool = False  # prefer an existing model over re-training\n",
    "    overwrite_results: bool = True  # if we do re-train, save it\n",
    "\n",
    "ip = InferenceParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_128, test_loader_128 = load_simulated_data(\n",
    "    path=ip.dataset_loc,\n",
    "    split_ratio=ip.split_ratio,\n",
    "    batch_size=128,\n",
    "    normalise_phot=normalise_phot_np,\n",
    "    transforms=[transforms.ToTensor()]\n",
    ")\n",
    "train_loader_512, test_loader_512 = load_simulated_data(\n",
    "    path=ip.dataset_loc,\n",
    "    split_ratio=ip.split_ratio,\n",
    "    batch_size=512,\n",
    "    normalise_phot=normalise_phot_np,\n",
    "    transforms=[transforms.ToTensor()]\n",
    ")\n",
    "train_loader_1024, test_loader_1024 = load_simulated_data(\n",
    "    path=ip.dataset_loc,\n",
    "    split_ratio=ip.split_ratio,\n",
    "    batch_size=1024,\n",
    "    normalise_phot=normalise_phot_np,\n",
    "    transforms=[transforms.ToTensor()]\n",
    ")\n",
    "logging.info('Data loading complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussians for Individual Parameters\n",
    "\n",
    "As a sanity-check to see whether there is useful signal in the data, here we attempt to maximise a Gaussian likelihood over just one parameter: $p(y_{i} | \\mathbf{x}) = \\mathcal{N}(y_{i}; \\mu, \\sigma^2)$. We set the parameters $\\theta = \\{\\mu, \\sigma\\}$ using a simple feed-forward ANN trained by minimising the NLL.\n",
    "\n",
    "For reference, available parameters are $y_{i}$ where $i \\in \\{0, \\ldots, 8\\}$ = {`redshift`, `log_mass`, `dust2`, `tage`, `log_tau`,\n",
    "        `log_agn_mass`, `agn_eb_v`, `log_agn_torus_mass`, `inclination`}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        net: list[nn.Module] = []\n",
    "        layer_sizes: list[int] = [8, 16, 2]\n",
    "        for l1, l2 in zip(layer_sizes, layer_sizes[1:]):\n",
    "            net.extend([nn.Linear(l1, l2), nn.BatchNorm1d(l2), nn.ReLU()])\n",
    "        net = net[:-2]  # remove final ReLU and BatchNorm1d\n",
    "        self.net: nn.Module = nn.Sequential(*net).to(device, dtype)\n",
    "        \n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        return self.net(x)\n",
    "    \n",
    "ann_param_idx = 0  # use the 1st dimension (arbitrarily); here redshift\n",
    "\n",
    "def train_network(train_loader: DataLoader, ip: InferenceParams) -> ANN:\n",
    "    savepath: str = './results/nbresults/ann.pt'\n",
    "    if not ip.retrain_model:\n",
    "        try:\n",
    "            logging.info(f'Attempting to load model from {savepath}')\n",
    "            net = t.load(savepath)\n",
    "            logging.info(f'Successfully loaded')\n",
    "            return net\n",
    "        except:\n",
    "            logging.info(f'No model {savepath} found; training...')\n",
    "    \n",
    "    net = ANN()\n",
    "    opt = t.optim.Adam(net.parameters(), lr=1e-3)\n",
    "    for e in range(ip.epochs):\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device, dtype), y.to(device, dtype)\n",
    "            params = net(x)\n",
    "            gaussian = Normal(params[:,0], params[:,1]**2)\n",
    "            loss = -gaussian.log_prob(y[:,ann_param_idx]).mean(0)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            if i % ip.logging_frequency == 0 or i == len(train_loader)-1:\n",
    "                logging.info(\n",
    "                    \"Epoch: {:02d}/{:02d}, Batch: {:03d}/{:d}, Loss {:9.4f}\"\n",
    "                    .format(e+1, ip.epochs, i, len(train_loader)-1, loss.item()))\n",
    "                \n",
    "    if ip.overwrite_results:\n",
    "        t.save(net, savepath)\n",
    "        logging.info(f'Saved model to {savepath}')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = train_network(train_loader_128, ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the labelscolumn_order\n",
    "param_label = column_order[ann_param_idx]\n",
    "\n",
    "xs, true_ys = nbu.new_sample(test_loader_128)\n",
    "xs = xs.to(device, dtype)\n",
    "true_ys = true_ys[ann_param_idx]\n",
    "\n",
    "net.eval()\n",
    "with t.inference_mode():\n",
    "    params = net(xs.unsqueeze(0)).squeeze()\n",
    "    mu, var = params[0].cpu().item(), t.exp(params[1]).cpu().item()\n",
    "density = t.distributions.Normal(mu, var)\n",
    "    \n",
    "plot_xs = t.linspace(-1, 2, 200)\n",
    "plot_ys = density.log_prob(plot_xs).exp().numpy()\n",
    "fig, ax = plt.subplots(figsize=(8, 4), dpi=200)\n",
    "ax.plot(plot_xs, plot_ys, label='log probability')\n",
    "ax.vlines(true_ys, 0, max(plot_ys), label='True parameter value', color='k')\n",
    "ax.legend()\n",
    "ax.set_xlabel(param_label + ' (normalised)')\n",
    "ax.set_ylabel(f'log probability of {param_label} value')\n",
    "ax.set_title('Single Gaussian Fit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post mortem\n",
    "\n",
    "This is clearly a poor model for the job. Nonetheless it does manage to learn adequate parameters for some parameters (the easiest to constrain seems to be the redshift). The predicted variance is often large when the mean is far from the true parameter value, as we might hope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Density Mixture Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the Gaussian with its light tails is not the best likelihood. Short of using a more robust likelihood such as a StudentT or Laplace distribution, we can modify the above to find the parameters of a mixture of Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDMN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # number of mixture components\n",
    "        K = 8\n",
    "        net: list[nn.Module] = []\n",
    "        layer_sizes: list[int] = [8, 16, 32]\n",
    "        for l1, l2 in zip(layer_sizes, layer_sizes[1:]):\n",
    "            net.extend([nn.Linear(l1, l2), nn.BatchNorm1d(l2), nn.ReLU()])\n",
    "        self.net: nn.Module = nn.Sequential(*net)\n",
    "        self.heads: nn.ModuleList = nn.ModuleList([\n",
    "            nn.Linear(layer_sizes[-1], K),\n",
    "            nn.Linear(layer_sizes[-1], K),\n",
    "            nn.Sequential(*[\n",
    "                nn.Linear(layer_sizes[-1], K),\n",
    "                nn.Softmax(dim=-1)\n",
    "            ])\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x: t.Tensor) -> list[t.Tensor]:\n",
    "        y = self.net(x)\n",
    "        return [h(y) for h in self.heads]\n",
    "    \n",
    "# still focusing on the redshift.\n",
    "gdmn_param_idx = 0\n",
    "\n",
    "def train_gdmn(train_loader: DataLoader, ip: InferenceParams) -> GDMN:\n",
    "    \n",
    "    savepath: str = './results/nbresults/gdmn.pt'\n",
    "    if not ip.retrain_model:\n",
    "        try:\n",
    "            logging.info(f'Attempting to load model from {savepath}')\n",
    "            gdmn = t.load(savepath).to(ip.device, ip.dtype)\n",
    "            logging.info(f'Successfully loaded')\n",
    "            return gdmn\n",
    "        except:\n",
    "            logging.info(f'No model {savepath} found; training...')\n",
    "    \n",
    "    gdmn = GDMN().to(device, dtype)\n",
    "    opt = t.optim.Adam(gdmn.parameters(), lr=1e-3)\n",
    "    for e in range(ip.epochs):\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device, dtype), y.to(device, dtype)\n",
    "            y = y[:,gdmn_param_idx].unsqueeze(-1)\n",
    "            \n",
    "            mu, sigma, alpha = gdmn(x)\n",
    "            normals = t.distributions.Normal(mu, squareplus_f(sigma))\n",
    "            LP = normals.log_prob(y)\n",
    "            loss = -(LP + alpha.log()).sum(1).mean(0)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            if i % ip.logging_frequency == 0 or i == len(train_loader)-1:\n",
    "                logging.info(\n",
    "                    \"Epoch: {:02d}/{:02d}, Batch: {:03d}/{:d}, Loss {:9.4f}\"\n",
    "                    .format(e+1, ip.epochs, i, len(train_loader)-1, loss.item()))\n",
    "                \n",
    "    if ip.overwrite_results:\n",
    "        t.save(gdmn, savepath)\n",
    "        logging.info(f'Saved model to {savepath}')\n",
    "        \n",
    "    return gdmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdmn = train_gdmn(train_loader_512, ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the labelscolumn_order\n",
    "param_label = column_order[gdmn_param_idx]\n",
    "\n",
    "xs, true_ys = nbu.new_sample(test_loader_512)\n",
    "xs = xs.to(device, dtype).unsqueeze(0)\n",
    "true_ys = true_ys[gdmn_param_idx]\n",
    "\n",
    "gdmn.eval()\n",
    "with t.inference_mode():\n",
    "    params = gdmn(xs)\n",
    "    mu, var, alpha = params[0].squeeze().cpu(), squareplus_f(params[1]).squeeze().cpu(), params[2].cpu()\n",
    "\n",
    "density = t.distributions.Normal(mu, var)\n",
    "plot_xs = t.linspace(0, 1, 200).unsqueeze(-1)\n",
    "plot_ys = (density.log_prob(plot_xs).exp() * alpha).sum(1).cpu().numpy()\n",
    "fig, ax = plt.subplots(figsize=(8, 4), dpi=200)\n",
    "ax.plot(plot_xs, plot_ys, label='log probability')\n",
    "ax.vlines(true_ys, 0, max(plot_ys), label='True parameter value', color='k')\n",
    "ax.legend()\n",
    "ax.set_xlabel(param_label +' (normalised)')\n",
    "ax.set_ylabel(f'log probability of {param_label} value')\n",
    "ax.set_title('Mixture of Gaussians')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post mortem\n",
    "\n",
    "This is already looking more promising than the single-Gaussian fit. The redshift is still the easiest parameter to constrain.\n",
    "\n",
    "When the real value is close to the mode of the distribution, the variance is also quite low, and when the true value lies away from the mode of the distribution, the variance is high, which is also fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variations on the above\n",
    "\n",
    "To try to understand the data better using our very simple model, we can vary the\n",
    "\n",
    "- batch size\n",
    "- epochs\n",
    "- learning rates\n",
    "- mixture densities\n",
    "- activation functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABInferenceParams(ConfigClass):\n",
    "    epochs: int = 3\n",
    "    batch_size: int = 1024\n",
    "    split_ratio: float = 0.9\n",
    "    dtype: t.dtype = dtype\n",
    "    device: t.device = device\n",
    "    logging_frequency: int = 2000\n",
    "    dataset_loc: str = './data/cubes/latest_sample/'\n",
    "    retrain_model: bool = False\n",
    "    overwrite_results: bool = True\n",
    "ab_ip = ABInferenceParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_train_loader, ab_test_loader = load_simulated_data(\n",
    "    path=ab_ip.dataset_loc,\n",
    "    split_ratio=ab_ip.split_ratio,\n",
    "    batch_size=ab_ip.batch_size,\n",
    "    normalise_phot=normalise_phot_np,\n",
    "    transforms=[transforms.ToTensor()]\n",
    ")\n",
    "logging.info('Data loading complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AB_GDMN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # number of mixture components\n",
    "        K = 6\n",
    "        net: list[nn.Module] = []\n",
    "        layer_sizes: list[int] = [8, 16, 32]\n",
    "        for l1, l2 in zip(layer_sizes, layer_sizes[1:]):\n",
    "            net.extend([nn.Linear(l1, l2), nn.BatchNorm1d(l2), nn.ReLU()])\n",
    "        self.net: nn.Module = nn.Sequential(*net)\n",
    "        self.heads: nn.ModuleList = nn.ModuleList([\n",
    "            nn.Linear(layer_sizes[-1], K),\n",
    "            nn.Linear(layer_sizes[-1], K),\n",
    "            nn.Sequential(*[\n",
    "                nn.Linear(layer_sizes[-1], K),\n",
    "                nn.Softmax(dim=-1)\n",
    "            ])\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x: t.Tensor) -> list[t.Tensor]:\n",
    "        y = self.net(x)\n",
    "        return [h(y) for h in self.heads]\n",
    "    \n",
    "\n",
    "# physical parameter index, no greater than 8\n",
    "# idx 4 is the log_tau parameter\n",
    "ab_param_idx = 4\n",
    "\n",
    "def train_ab_gdmn(train_loader: DataLoader, ip: ABInferenceParams) -> AB_GDMN:\n",
    "                  # epochs: int = ab_ip.epochs, log_every: int = ab_ip.logging_frequency):\n",
    "    savepath: str = './results/nbresults/ab_gdmn.pt'\n",
    "    if not ip.retrain_model:\n",
    "        try:\n",
    "            logging.info(f'Attempting to load model from {savepath}')\n",
    "            ab_gdmn = t.load(savepath).to(ip.device, ip.dtype)\n",
    "            logging.info(f'Successfully loaded')\n",
    "            return ab_gdmn\n",
    "        except:\n",
    "            logging.info(f'No model {savepath} found; training...')\n",
    "    \n",
    "    ab_gdmn = AB_GDMN().to(ip.device, ip.dtype)\n",
    "    opt = t.optim.Adam(ab_gdmn.parameters(), lr=1e-3)\n",
    "    ab_gdmn.train()\n",
    "    for e in range(ip.epochs):\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device, dtype), y.to(device, dtype)\n",
    "            y = y[:, ab_param_idx].unsqueeze(-1)\n",
    "            \n",
    "            mu, scale, alpha = ab_gdmn(x)\n",
    "            normals = t.distributions.StudentT(1., mu, squareplus_f(scale))\n",
    "            LP = normals.log_prob(y)\n",
    "            loss = -(LP + alpha).sum(1).mean(0)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            if i % ip.logging_frequency == 0 or i == len(train_loader)-1:\n",
    "                logging.info(\n",
    "                    \"Epoch: {:02d}/{:02d}, Batch: {:03d}/{:d}, Loss {:9.4f}\"\n",
    "                    .format(e+1, ip.epochs, i, len(train_loader)-1, loss.item()))\n",
    "                \n",
    "    if ip.overwrite_results:\n",
    "        t.save(ab_gdmn, savepath)\n",
    "        logging.info(f'Saved model to {savepath}')\n",
    "        \n",
    "    return ab_gdmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_gdmn = train_ab_gdmn(train_loader_1024, ab_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the labelscolumn_order\n",
    "param_label = column_order[ab_param_idx]\n",
    "\n",
    "xs, true_ys = nbu.new_sample(test_loader_1024)\n",
    "xs = xs.to(device, dtype).unsqueeze(0)\n",
    "true_ys = true_ys[ab_param_idx]\n",
    "\n",
    "ab_gdmn.eval()\n",
    "with t.inference_mode():\n",
    "    params = ab_gdmn(xs)\n",
    "    loc, scale, alpha = params[0].squeeze().cpu(), squareplus_f(params[1]).squeeze().cpu(), params[2].cpu()\n",
    "\n",
    "# assume 1 dof\n",
    "density = t.distributions.StudentT(1., loc, scale)\n",
    "\n",
    "plot_xs = t.linspace(0, 1, 200).unsqueeze(-1)\n",
    "plot_ys = density.log_prob(plot_xs).exp()\n",
    "plot_ys = (plot_ys * alpha).sum(1).cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4), dpi=200)\n",
    "ax.plot(plot_xs, plot_ys, label='log probability')\n",
    "ax.vlines(true_ys, 0, max(plot_ys), label='True parameter value', color='k')\n",
    "ax.legend()\n",
    "ax.set_xlabel(param_label +' (normalised)')\n",
    "ax.set_ylabel(f'log probability of {param_label} value')\n",
    "ax.set_title('Maximising StudentT likelihood for hard-to-constrain parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "While the `log_tau` parameter, is harder to constrain than the `redshift`, by maximising the likelihood of a heavy tailed distribution like the StudentT, we get stable training and reasonable (if over-simplistic) posterior estimates.\n",
    "\n",
    "Changing the mixture density from a Gaussian to a more robust distribution like the StudentT helped to bring down the NLL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Mixture Distributions\n",
    "\n",
    "With reasonable success treating variables individually, let's learn multiple at the same time. This should be identical to the above when looking at the individual parameters. We begin with the humble factorised Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture of Factorised Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGMM(nn.Module):\n",
    "    def __init__(self, K: int = 5, D: int = 2) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            K: the number of mixture components to use\n",
    "            D: the number of dimensions of the individual Gaussians.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.D = D\n",
    "        \n",
    "        net: list[nn.Module] = []\n",
    "        layer_sizes: list[int] = [8, 16, 32] # TODO: try something a little bit bigger.\n",
    "        for l1, l2 in zip(layer_sizes, layer_sizes[1:]):\n",
    "            net.extend([nn.Linear(l1, l2), nn.BatchNorm1d(l2), nn.ReLU()])\n",
    "        self.net: nn.Module = nn.Sequential(*net)\n",
    "        self.heads: nn.ModuleList = nn.ModuleList([\n",
    "            nn.Linear(layer_sizes[-1], K*D), # mean vectors\n",
    "            nn.Linear(layer_sizes[-1], K*D), # covariance matrix diagonals\n",
    "            nn.Sequential(*[nn.Linear(layer_sizes[-1], K),nn.Softmax(dim=-1)]) # mixture weights\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x: t.Tensor) -> list[t.Tensor]:\n",
    "        y = self.net(x)\n",
    "        hs = [h(y) for h in self.heads]\n",
    "        eyes = t.eye(self.D, dtype=dtype, device=device).repeat(x.size(0), self.K, 1)\n",
    "        diags = eyes * squareplus_f(hs[1]).unsqueeze(-1)\n",
    "        diags = diags.reshape(-1, self.K, self.D, self.D)\n",
    "        \n",
    "        # return mean vectors, lower-triangular matrices, and mixture weights\n",
    "        return [hs[0].reshape(-1, self.K, self.D), diags, hs[2]]\n",
    "    \n",
    "fgmm_param_idxs: list[int] = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "K = 3\n",
    "D = len(fgmm_param_idxs)\n",
    "\n",
    "def train_fgmm(train_loader: DataLoader, ip: InferenceParams) -> FGMM:\n",
    "        \n",
    "    savepath: str = './results/nbresults/fgmm.pt'\n",
    "    if not ip.retrain_model:\n",
    "        try:\n",
    "            logging.info(f'Attempting to load model from {savepath}')\n",
    "            fgmm = t.load(savepath).to(ip.device, ip.dtype)\n",
    "            logging.info(f'Successfully loaded')\n",
    "            return fgmm\n",
    "        except:\n",
    "            logging.info(f'No model {savepath} found; training...')\n",
    "    \n",
    "    fgmm = FGMM(K, D).to(ip.device, ip.dtype)\n",
    "    opt = t.optim.Adam(fgmm.parameters(), lr=1e-3)\n",
    "    fgmm.train()\n",
    "    for e in range(ip.epochs):\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            \n",
    "            x, y = x.to(device, dtype), y.to(device, dtype)\n",
    "            y = y[:, fgmm_param_idxs]\n",
    "            if D == 1:\n",
    "                y = y[:,None]\n",
    "            \n",
    "            locs, Ls, alpha = fgmm(x)\n",
    "            normals = MultivariateNormal(locs, scale_tril=Ls)\n",
    "            P = normals.log_prob(y.unsqueeze(-2))\n",
    "            loss = -(P + alpha.log()).sum(1).mean(0)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            if i % ip.logging_frequency == 0 or i == len(train_loader)-1:\n",
    "                logging.info(\n",
    "                    \"Epoch: {:02d}/{:02d}, Batch: {:03d}/{:d}, Loss {:9.4f}\"\n",
    "                    .format(e+1, ip.epochs, i, len(train_loader)-1, loss.item()))\n",
    "                \n",
    "    if ip.overwrite_results:\n",
    "        t.save(fgmm, savepath)\n",
    "        logging.info(f'Saved model to {savepath}')\n",
    "\n",
    "    return fgmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgmm = train_fgmm(train_loader_1024, ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [column_order[i] for i in fgmm_param_idxs]\n",
    "\n",
    "xs, true_ys = nbu.new_sample(test_loader_1024)\n",
    "xs = xs.to(device, dtype).unsqueeze(0)\n",
    "true_ys = t.tensor([true_ys[i] for i in fgmm_param_idxs])\n",
    "\n",
    "fgmm.eval()\n",
    "with t.inference_mode():\n",
    "    locs, Ls, alpha = fgmm(xs)\n",
    "    \n",
    "mix = Categorical(alpha)\n",
    "normals = MultivariateNormal(locs, scale_tril=Ls)\n",
    "mixture = MixtureSameFamily(mix, normals)\n",
    "\n",
    "n_samples = 10000\n",
    "samples = mixture.sample((n_samples,)).cpu().squeeze(1)\n",
    "# samples = normals.sample((n_samples,)).squeeze(1)\n",
    "\n",
    "lims = np.array([[0.,1.]]).repeat(len(params),0)\n",
    "nbu.plot_corner(samples.numpy(), true_ys.cpu().numpy(), lims=lims,\n",
    "               title=\"Mixture of Factorised Gaussians\",\n",
    "               description=\"3 mixture components, trained on full dataset, batch size 1024, 5 epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Mortem\n",
    "\n",
    "The above matches what we would expect:\n",
    "\n",
    "- the joint 'ellipses' are orthogonal to the axes due to the factorisation\n",
    "- some parameters (e.g. redshift, masses) are easier to constrain than others (tage, inclination...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full-Covariance Gaussians\n",
    "\n",
    "Trying to move beyond the factorised case, can our simple model learn joint interactions between the physical parameter values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVGMN(nn.Module):\n",
    "    def __init__(self, K: int = 5, D: int = 2) -> None:\n",
    "        \"\"\"Multivaritate Gaussian mixture network\n",
    "        \n",
    "        Args:\n",
    "            K: the number of mixture components to use\n",
    "            D: the number of dimensions of the individual Gaussians.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.D = D\n",
    "        \n",
    "        net: list[nn.Module] = []\n",
    "        layer_sizes: list[int] = [8, 16, 32]\n",
    "        for l1, l2 in zip(layer_sizes, layer_sizes[1:]):\n",
    "            net.extend([nn.Linear(l1, l2), nn.BatchNorm1d(l2), nn.ReLU()])\n",
    "        self.net: nn.Module = nn.Sequential(*net)\n",
    "        self.heads: nn.ModuleList = nn.ModuleList([\n",
    "            nn.Linear(layer_sizes[-1], K*D), # mean vectors\n",
    "            nn.Linear(layer_sizes[-1], K*D), # covariance matrix diagonals\n",
    "            nn.Linear(layer_sizes[-1], K*self.tril_size(D)), # terms of the lower-triangular matrix\n",
    "            nn.Sequential(*[nn.Linear(layer_sizes[-1], K), nn.Softmax(dim=-1)]) # mixture weights\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x: t.Tensor) -> list[t.Tensor]:\n",
    "        y = self.net(x)\n",
    "        hs = [h(y) for h in self.heads]\n",
    "        \n",
    "        # Create covariance matrix\n",
    "        # 1. create [N, K] diagonal matrices of side length D with positive covariances\n",
    "        eyes = t.eye(self.D, dtype=dtype, device=device).repeat(x.size(0), self.K, 1)\n",
    "        diags = eyes * squareplus_f(hs[1]).unsqueeze(-1)\n",
    "        diags = diags.reshape(-1, self.K, self.D, self.D)\n",
    "        \n",
    "        # 2. fill in the lower-triangular sections if D > 1\n",
    "        if self.D > 1:\n",
    "            ti = t.tril_indices(self.D, self.D, -1)\n",
    "            diags[:,:,ti[0],ti[1]] = hs[2].reshape(x.size(0), self.K, -1)\n",
    "        \n",
    "        # return mean vectors, lower-triangular matrices, and mixture weights\n",
    "        return [hs[0].reshape(-1, self.K, self.D), diags, hs[3]]\n",
    "    \n",
    "    def tril_size(self, N: int) -> int:\n",
    "        return int((N**2 - N)/2)\n",
    "    \n",
    "mvgmn_param_idxs: list[int] = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "K = 3\n",
    "D =len(mvgmn_param_idxs)\n",
    "\n",
    "def train_mvgmn(train_loader: DataLoader, ip: InferenceParams) -> MVGMN:\n",
    "    \n",
    "    savepath: str = './results/nbresults/mvgmn.pt'\n",
    "    if not ip.retrain_model:\n",
    "        try:\n",
    "            logging.info(f'Attempting to load model from {savepath}')\n",
    "            mvgmm = t.load(savepath).to(ip.device, ip.dtype)\n",
    "            logging.info(f'Successfully loaded')\n",
    "            return mvgmm\n",
    "        except:\n",
    "            logging.info(f'No model {savepath} found; training...')\n",
    "    \n",
    "    mvgmn = MVGMN(K, D).to(ip.device, ip.dtype)\n",
    "    opt = t.optim.Adam(mvgmn.parameters(), lr=1e-3)\n",
    "    mvgmn.train()\n",
    "    for e in range(ip.epochs):\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            \n",
    "            x, y = x.to(device, dtype), y.to(device, dtype)\n",
    "            y = y[:, mvgmn_param_idxs]\n",
    "            if D == 1:\n",
    "                y = y[:,None]\n",
    "            \n",
    "            locs, Ls, alpha = mvgmn(x)\n",
    "            normals = MultivariateNormal(locs, scale_tril=Ls)\n",
    "            P = normals.log_prob(y.unsqueeze(-2))\n",
    "            loss = -(P + alpha.log()).sum(1).mean(0)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            if i % ip.logging_frequency == 0 or i == len(train_loader)-1:\n",
    "                logging.info(\n",
    "                    \"Epoch: {:02d}/{:02d}, Batch: {:03d}/{:d}, Loss {:9.4f}\"\n",
    "                    .format(e+1, ip.epochs, i, len(train_loader)-1, loss.item()))\n",
    "            \n",
    "    if ip.overwrite_results:\n",
    "        t.save(mvgmn, savepath)\n",
    "        logging.info(f'Saved model to {savepath}')\n",
    "        \n",
    "    return mvgmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvgmn = train_mvgmn(train_loader_1024, ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [column_order[i] for i in mvgmn_param_idxs]\n",
    "\n",
    "xs, true_ys = nbu.new_sample(test_loader_1024)\n",
    "xs = xs.to(device, dtype).unsqueeze(0)\n",
    "true_ys = t.tensor([true_ys[i] for i in mvgmn_param_idxs])\n",
    "\n",
    "mvgmn.eval()\n",
    "with t.inference_mode():\n",
    "    locs, Ls, alpha = mvgmn(xs)\n",
    "normals = MultivariateNormal(locs, scale_tril=Ls)\n",
    "\n",
    "samples = normals.sample((10000,)).squeeze(1).mean(1).cpu()\n",
    "\n",
    "lims = np.array([[0.,1.]]).repeat(len(params),0)\n",
    "nbu.plot_corner(samples.numpy(), true_ys.cpu().numpy(), lims=lims,\n",
    "               title=\"Mixture of Gaussians\",\n",
    "               description=\"3 mixture components, trained on full dataset, batch size 1024, 10 epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Mortem\n",
    "\n",
    "The accurcy of the model seems to have suffered somewhat compared to the factorised Gaussian. This is perhaps due to the larger number of parameters we now need to learn, and so training this for longer could provide a rudimentary solution.\n",
    "\n",
    "Not much was gained since the joints mostly remain independent apart from a few sporadic exceptions. The above took about half an hour to train and despite this, the marginals are overly-simplistic and not very accurate.\n",
    "\n",
    "This is probably the extent of the usefulness of simple feed-forward networks parametrising mixture distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Sequential Autoregressive Network\" (SAN)\n",
    "\n",
    "If $\\mathbf{y} \\in \\mathbb{R}^{D}$ and $\\mathbf{x} \\in \\mathbb{R}^{E}$ then we can write down the joint as\n",
    "\\begin{align*}\n",
    "p(\\mathbf{x}, \\mathbf{y}) &= p(y_{1} \\vert \\mathbf{x})p(y_{2}\\vert y_{1}, \\mathbf{x})\\cdots p(y_{D}\\vert y_{1}, \\ldots, y_{D-1}, \\mathbf{x}) \\\\\n",
    "&= \\prod^{D}_{d=1}p(y_{d} \\vert \\mathbf{y}_{<d}, \\mathbf{x}).\n",
    "\\end{align*}\n",
    "\n",
    "This is the _autoregressive property_ that is exploited by models like MADE or Masked Autoregressive Flows. Any architecture satisfying this autoregressive property can straightforwardly be trained by minimising the resulting NLL:\n",
    "\n",
    "$$\n",
    "\\ell(\\mathbf{y}; \\mathbf{x}) = - \\sum^{D}_{d=1} \\log p(y_d \\vert \\mathbf{y}_{<d}, \\mathbf{x}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following architecture satisfies this property by explicitly modelling each $y_{d}, d \\in \\{1, \\ldots, D\\}$ sequentially and conditioning appropriately. It is formed of $D$ \"_sequential blocks_\", which are identical layers (which do not share weights) and are intended to learn features which allow the parameters of the $d$'th parameter, $\\theta_{d}$ to be determined; $y_{d} \\sim p(y_{d}; \\theta_{d})$.\n",
    "\n",
    "Note that the likelihood needn't be Gaussian. We can choose to output an arbitrary number of parameters $\\theta_{d}$ from each sequential block, and use these in arbitrary density functions. For instance, we could parametrise a mixture distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(SVG(filename='./notebooks/inference/san.svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SANParams(ConfigClass):\n",
    "    cond_dim: int = 8  # dimensions of conditioning info (e.g. photometry)\n",
    "    data_dim: int = 9  # dimensions of data of interest (e.g. physical params)\n",
    "    module_shape: list[int] = [16, 16]  # shape of the network 'modules'\n",
    "    batch_norm: bool = True  # use batch normalisation in network?\n",
    "sp = SANParams() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_san(model: SAN, sp: SANParams, ip: InferenceParams) -> SAN:\n",
    "    san = model(sp.cond_dim, sp.data_dim, sp.module_shape, \n",
    "                       device=ip.device, dtype=ip.dtype)\n",
    "\n",
    "    savepath: str = san.fpath()\n",
    "    if not ip.retrain_model:\n",
    "        try:\n",
    "            logging.info(f'Attempting to load {san.likelihood_name} SAN model from {savepath}')\n",
    "            san = t.load(savepath).to(ip.device, ip.dtype)\n",
    "            logging.info(f'Successfully loaded')\n",
    "            return san.cuda() if ip.device == t.device('cuda') else san\n",
    "        except:\n",
    "            logging.info(f'No model {savepath} found; training...')\n",
    "            \n",
    "    san.trainmodel(train_loader_1024, ip.epochs, ip.logging_frequency)\n",
    "    logging.info(f'Trained {san.likelihood_name} SAN model')\n",
    "\n",
    "    t.save(san, san.fpath())\n",
    "    logging.info(f'Saved {san.likelihood_name} SAN model as: {san.fpath()}')\n",
    "    return san.cuda() if ip.device == t.device('cuda') else san"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian SAN\n",
    "\n",
    "A sequential autoregressive network with Gaussian likelihoods: $p(y_{d} \\vert \\mathbf{y}_{<d}, \\mathbf{x}) = \\mathcal{N}(y_{d}; \\mu_{d}, \\sigma^{2}_{d})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsan = train_san(Gaussian_SAN, sp, ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, true_ys = nbu.new_sample(test_loader_1024)\n",
    "xs = xs.to(device, dtype)\n",
    "true_ys = true_ys.to(device, dtype)\n",
    "# true_ys = t.tensor([true_ys[i] for i in fgmm_param_idxs])\n",
    "\n",
    "n_samples = 10000\n",
    "with t.inference_mode():\n",
    "    start = time.time()\n",
    "    samples = gsan.sample(xs, n_samples=n_samples).cpu()\n",
    "    sampling_time = (time.time() - start) * 1e3\n",
    "logging.info(f'Finished drawing {n_samples:,} samples in {sampling_time:.4f}ms.')\n",
    "logging.info('Plotting results...')\n",
    "\n",
    "description = f'{gsan} trained for {ip.epochs} epochs (batch size {ip.batch_size})'\n",
    "\n",
    "lims = np.array([[0.,1.]]).repeat(len(column_order),0)\n",
    "nbu.plot_corner(samples=samples.numpy(), true_params=true_ys.cpu().numpy(), \n",
    "                lims=lims, labels=column_order, title='Gaussian \"Sequential Autoregressive Network\"',\n",
    "                description=description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Mortem\n",
    "\n",
    "Training this model takes longer than e.g. MADE, however the sampling procedure is fairly quick (e.g. 10,000 samples in 2.3ms). This model becomes relatively expensive for higher dimensional data, since the 'sequential block' must be repeated for each new output dimension; adding additional parameters to the model, which must be evaluated sequentially (i.e. the network becomes deeper, not wider, and this sequential computation is bad for parallelisation).\n",
    "\n",
    "The conditional distributions additionally seem quite concentrated in instances where they perhaps should not be. This may be due to the light tails on the Gaussian likelihood. Below we attempt the same thing, this time using a Laplace likelihood:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace SAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsan = train_san(Laplace_SAN, sp, ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, true_ys = nbu.new_sample(test_loader_1024)\n",
    "xs = xs.to(device, dtype)\n",
    "true_ys = true_ys.to(device, dtype)\n",
    "# true_ys = t.tensor([true_ys[i] for i in fgmm_param_idxs])\n",
    "\n",
    "n_samples = 10000\n",
    "with t.inference_mode():\n",
    "    start = time.time()\n",
    "    samples = lsan.sample(xs, n_samples=n_samples).cpu()\n",
    "    sampling_time = (time.time() - start) * 1e3\n",
    "logging.info(f'Finished drawing {n_samples:,} samples in {sampling_time:.4f}ms.')\n",
    "logging.info('Plotting results...')\n",
    "\n",
    "description = f'{lsan} trained for {ip.epochs} epochs (batch size {ip.batch_size})'\n",
    "\n",
    "lims = np.array([[0.,1.]]).repeat(len(column_order),0)\n",
    "nbu.plot_corner(samples=samples.numpy(), true_params=true_ys.cpu().numpy(), \n",
    "                lims=lims, labels=column_order, title='Laplace \"Sequential Autoregressive Network\"',\n",
    "                description=description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MADE\n",
    "\n",
    "**The model used in the following cells has a (known) bug to do with the mask sampling.**\n",
    "\n",
    "This means that some dimensions do not satisfy the autoregressive property (i.e. are conditioned on 'too much'), and others are not conditioned on anything! The only value that these cells bring at the moment is to compare the speed of training and inference against e.g. the \"sequential autoregressive network\" above.\n",
    "\n",
    "First, we try MADE with a single mask / input ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADEInferenceParameters(ConfigClass):\n",
    "    epochs: int = 5\n",
    "    batch_size: int = 1024\n",
    "    split_ratio: float = 0.9\n",
    "    dtype: t.dtype = dtype\n",
    "    device: t.device = device\n",
    "    logging_frequency: int = 10000\n",
    "    dataset_loc: str = './data/cubes/40M_shuffled.hdf5'\n",
    "    retrain_model: bool = False\n",
    "    overwrite_results: bool = True\n",
    "made_ip = MADEInferenceParameters()\n",
    "\n",
    "class MADEParams(ConfigClass):\n",
    "    cond_dim: int = 8  # x; dimensions of photometry\n",
    "    data_dim: int = 9  # y; dimensions of physical parameters to be estimated\n",
    "    hidden_sizes: list[int] = [128, 128]\n",
    "    likelihood: Type[made.MADE_Likelihood] = made.Gaussian\n",
    "    likelihood_kwargs = None\n",
    "\n",
    "    # maximum number of different masks / orderings for connectivity / order agnostic training\n",
    "    num_masks: int = 128\n",
    "    \n",
    "    conditional_all: bool = False\n",
    "\n",
    "    # The number of samples of masks to average parameters over for each \n",
    "    # training iteration. \n",
    "    samples: int = 16\n",
    "\n",
    "    # whether to factorise the joint data distribution in the same order as the\n",
    "    # dimensions are naturally given.\n",
    "    natural_ordering: bool = False\n",
    "mp = MADEParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mademodel = CMADE(cond_dim=mp.cond_dim, data_dim=mp.data_dim,\n",
    "                  hidden_sizes=mp.hidden_sizes,\n",
    "                  likelihood=mp.likelihood,\n",
    "                  likelihood_kwargs=mp.likelihood_kwargs,\n",
    "                  num_masks=mp.num_masks, natural_ordering=mp.natural_ordering,\n",
    "                  device=made_ip.device, dtype=made_ip.dtype)\n",
    "if made_ip.device == t.device('cuda'):\n",
    "    mademodel = mademodel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "made_train_loader, made_test_loader = load_simulated_data(\n",
    "    path=made_ip.dataset_loc,\n",
    "    split_ratio=made_ip.split_ratio,\n",
    "    batch_size=made_ip.batch_size,\n",
    "    normalise_phot=normalise_phot_np,\n",
    "    transforms=[transforms.ToTensor()]\n",
    ")\n",
    "logging.info('Data loading complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mademodel.trainmodel(made_train_loader, made_ip.epochs, mp.samples, made_ip.logging_frequency)\n",
    "logging.info(\"Trained MADE model\")\n",
    "\n",
    "t.save(mademodel, mademodel.fpath())\n",
    "logging.info(f'Saved MADE model as: {mademodel.fpath()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mademodel = t.load(mademodel.fpath())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, true_ys = nbu.new_sample(made_test_loader)\n",
    "xs = xs.to(device, dtype).unsqueeze(0)\n",
    "true_ys = true_ys.to(device, dtype)\n",
    "\n",
    "mask_idxs = t.randperm(mademodel.num_masks)[:1]\n",
    "n_samples = 10000\n",
    "mademodel.eval()\n",
    "with t.no_grad():\n",
    "    start = time.time()\n",
    "    samples = mademodel.sample(xs, n_samples=n_samples, mask_idxs=mask_idxs).cpu()\n",
    "    sampling_time = (time.time() - start) * 1e3\n",
    "logging.info(f'Finished drawing {n_samples:,} samples in {sampling_time:.4f}ms.')\n",
    "logging.info('Plotting results...')\n",
    "\n",
    "description = f'Trained {mademodel} for {made_ip.epochs} epochs (batch size {made_ip.batch_size})'\n",
    "\n",
    "lims = np.array([[0.,1.]]).repeat(len(column_order),0)\n",
    "nbu.plot_corner(samples=samples.numpy(), true_params=true_ys.cpu().numpy(), \n",
    "                lims=lims, labels=column_order, title=\"MADE\",\n",
    "                description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, true_ys = nbu.new_sample(made_test_loader, 10000)\n",
    "xs = xs.to(device, dtype)\n",
    "true_ys = true_ys.to(device, dtype)\n",
    "\n",
    "mask_idxs = t.randperm(made.num_masks)[:1]\n",
    "mask_idxs = None\n",
    "\n",
    "# samples per posterior\n",
    "N = 1000\n",
    "\n",
    "samples = t.empty((0, 9)).to(dtype=dtype)\n",
    "sample_list: list[t.Tensor] = []\n",
    "\n",
    "with t.inference_mode():\n",
    "    sample_list = [made.sample(x.unsqueeze(0), n_samples=N, mask_idxs=mask_idxs).cpu() for x in xs]\n",
    "samples = t.cat(sample_list, 0)\n",
    "\n",
    "description = f'Trained {made} for {made_ip.epochs} epochs (batch size {made_ip.batch_size})'\n",
    "        \n",
    "true_ys = true_ys.repeat_interleave(N, 0).cpu().numpy()\n",
    "nbu.plot_posteriors(samples.cpu().numpy(), true_ys, title=\"MADE\",\n",
    "                   description=description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Mortem\n",
    "\n",
    "This is clearly finding some signal in the data, although perhaps not as much as we would like. What is the effect of ensembling over more masks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADEParams2(ConfigClass):\n",
    "    cond_dim: int = 8  # x; dimensions of photometry\n",
    "    data_dim: int = 9  # y; dimensions of physical parameters to be estimated\n",
    "    hidden_sizes: list[int] = [32, 32]\n",
    "\n",
    "    # maximum number of different masks / orderings for connectivity / order agnostic training\n",
    "    num_masks: int = 128\n",
    "\n",
    "    # The number of samples of masks to average parameters over for each \n",
    "    # training iteration. \n",
    "    samples: int = 16\n",
    "\n",
    "    # whether to factorise the joint data distribution in the same order as the\n",
    "    # dimensions are naturally given.\n",
    "    natural_ordering: bool = False\n",
    "mp2 = MADEParams2()\n",
    "\n",
    "made2 = CMADE(cond_dim=mp2.cond_dim, data_dim=mp2.data_dim,\n",
    "             hidden_sizes=mp2.hidden_sizes, out_size=2*mp2.data_dim,\n",
    "             num_masks=mp2.num_masks, natural_ordering=mp2.natural_ordering,\n",
    "             device=made_ip.device, dtype=made_ip.dtype)\n",
    "if made_ip.device == t.device('cuda'):\n",
    "    made2 = made2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "made2.trainmodel(made_train_loader, made_ip.epochs, mp2.samples, made_ip.logging_frequency)\n",
    "logging.info(\"Trained MADE model\")\n",
    "\n",
    "t.save(made2, made2.fpath())\n",
    "logging.info(f'Saved MADE model as: {made2.fpath()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "made2 = t.load(made2.fpath())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, true_ys = nbu.new_sample(made_test_loader)\n",
    "xs = xs.to(device, dtype).unsqueeze(0)\n",
    "true_ys = true_ys.to(device, dtype)\n",
    "\n",
    "mask_idxs = t.randperm(made2.num_masks)[:30]\n",
    "# mask_idxs = None\n",
    "\n",
    "n_samples = 10000\n",
    "with t.inference_mode():\n",
    "    start = time.time()\n",
    "    samples = made2.sample(xs, n_samples=n_samples, mask_idxs=mask_idxs).cpu()\n",
    "    sampling_time = (time.time() - start) * 1e3\n",
    "logging.info(f'Finished drawing {n_samples:,} samples in {sampling_time:.4f}ms.')\n",
    "logging.info('Plotting results...')\n",
    "\n",
    "description = f'Trained {made2} for {made_ip.epochs} epochs (batch size {made_ip.batch_size})'\n",
    "\n",
    "lims = np.array([[0.,1.]]).repeat(len(column_order),0)\n",
    "# lims = None\n",
    "nbu.plot_corner(samples=samples.numpy(), true_params=true_ys.cpu().numpy(), \n",
    "                lims=lims, labels=column_order, title=\"MADE with Mask Ensemble\",\n",
    "                description=description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this is not very instructive because this is an ensemble of faulty masks - which makes the plot above, to put it nicely, garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, true_ys = nbu.new_sample(made_test_loader, 10000)\n",
    "xs = xs.to(device, dtype)\n",
    "true_ys = true_ys.to(device, dtype)\n",
    "\n",
    "# mask_idxs = t.randperm(made.num_masks)\n",
    "mask_idxs = None\n",
    "\n",
    "# samples per posterior\n",
    "N = 1000\n",
    "\n",
    "samples = t.empty((0, 9)).to(dtype=dtype)\n",
    "sample_list: list[t.Tensor] = []\n",
    "\n",
    "with t.inference_mode():\n",
    "    sample_list = [made2.sample(x.unsqueeze(0), n_samples=N, mask_idxs=mask_idxs).cpu() for x in xs]\n",
    "samples = t.cat(sample_list, 0)\n",
    "\n",
    "description = f'Trained {made2} for {made_ip.epochs} epochs (batch size {made_ip.batch_size})'\n",
    "        \n",
    "true_ys = true_ys.repeat_interleave(N, 0).cpu().numpy()\n",
    "nbu.plot_posteriors(samples.cpu().numpy(), true_ys, title=\"MADE\",\n",
    "                   description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agnvenv (Python 3.9)",
   "language": "python",
   "name": "agnvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
